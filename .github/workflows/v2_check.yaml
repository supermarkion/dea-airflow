---
name: v2.1 DAGs unit tests

on:
  push:
    paths:
      - 'dags/**'
      - 'scripts/**'
      - 'tests/**'
      - 'plugins/**'
      - '.github/workflows/**'
      - 'requirements-dev.txt'
      - 'requirements.txt'
      - 'constraints.txt'
  pull_request:
    types: [opened, reopened]
    branches:
      - master
      - develop
    paths:
      - 'dags/**'
      - 'scripts/**'
      - 'tests/**'
      - 'plugins/**'
      - '.github/workflows/**'
      - 'requirements-dev.txt'
      - 'requirements.txt'
      - 'constraints.txt'

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8]
        airflow-version: [2.1.2]
    name: Airflow ${{ matrix.airflow-version }}
    steps:
      - uses: actions/checkout@v2
      - name: Install Airflow and Lint
        uses: s-weigand/setup-conda@v1
        with:
          update-conda: true
          python-version: ${{ matrix.python-version }}
          conda-channels: anaconda, conda-forge
      - name: Check conda version
        run: conda --version
      - name: Check python
        run: which python
      - name: Install airflow and run pylint
        run: |
          pip install apache-airflow[amazon,cncf.kubernetes,sftp,postgres,redis,ssh,celery,http]==${{ matrix.airflow-version }} -c constraints.txt
          pip install airflow-kubernetes-job-operator SQLAlchemy==1.3.23 boto3 kubernetes cryptography
          pip install pylint pylint-airflow
          pylint --load-plugins=pylint_airflow --disable=C,W --disable=similarities dags
      - name: setup airflow and check dags list
        run: |
          export AIRFLOW__CORE__LOAD_EXAMPLES="False"
          export AIRFLOW__CORE__PLUGINS_FOLDER="/home/runner/work/dea-airflow/dea-airflow/plugins"
          export AIRFLOW__CORE__DAGS_FOLDER="/home/runner/work/dea-airflow/dea-airflow/dags"

          airflow version
          echo "y" | airflow db reset
          airflow db init
          airflow connections add 'aws_nci_db_backup' --conn-type 'aws' --conn-login 'AWSKEY' --conn-password 'AWS_PASS'
          airflow connections add 'lpgs_gadi' --conn-type 'ssh' --conn-login 'USERNAME' --conn-password 'PASS'
          airflow info
          airflow dags list

          # check if dags list has error if yes exit CI
          daglist=`airflow dags list`
          errorstring="Error"
          if [[ $daglist == *"$errorstring"* ]]; then
              echo "error in dags"
              exit 1
          fi

      - name: run tests on dags
        run: |
          export PYTHONPATH="${PYTHONPATH:+${PYTHONPATH}:}${PWD}/dags"
          pip install pylint==2.7.2 pytest==6.2.2 pylint-airflow
          pytest tests/dag_structure