---
name: v2.1 DAGs unit tests

on:
  push:
    paths:
      - 'dags/**'
      - 'scripts/**'
      - 'tests/**'
      - 'plugins/**'
      - '.github/workflows/**'
      - 'requirements-dev.txt'
      - 'requirements.txt'
      - 'constraints.txt'
  pull_request:
    types: [opened, reopened]
    branches:
      - master
      - develop
    paths:
      - 'dags/**'
      - 'scripts/**'
      - 'tests/**'
      - 'plugins/**'
      - '.github/workflows/**'
      - 'requirements-dev.txt'
      - 'requirements.txt'
      - 'constraints.txt'

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8]
        airflow-version: [2.2.2]
    name: Airflow ${{ matrix.airflow-version }}
    steps:
      - name: checkout git
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
      - name: Install Airflow and Lint
        uses: s-weigand/setup-conda@v1
        with:
          update-conda: true
          python-version: ${{ matrix.python-version }}
          conda-channels: anaconda, conda-forge
      - name: Check conda version
        run: conda --version
      - name: Check python
        run: which python
      - name: Install airflow and run pylint
        run: |
          pip install apache-airflow[amazon,cncf.kubernetes,sftp,postgres,redis,ssh,celery,http]==${{ matrix.airflow-version }} -c "https://raw.githubusercontent.com/apache/airflow/constraints-2.2.2/constraints-3.7.txt"
          pip install -r requirements.txt -c constraints.txt
          pip install pylint pylint-airflow -c constraints.txt
          pylint --load-plugins=pylint_airflow --disable=C,W --disable=similarities dags
      - name: setup airflow and check dags list
        run: |
          airflow db upgrade
          airflow db init

          export AIRFLOW__CORE__LOAD_EXAMPLES="False"
          export AIRFLOW__CORE__PLUGINS_FOLDER="/home/runner/work/dea-airflow/dea-airflow/plugins"
          export AIRFLOW__CORE__DAGS_FOLDER="/home/runner/work/dea-airflow/dea-airflow/dags"

          airflow connections add 'aws_nci_db_backup' --conn-type 'aws' --conn-login 'AWSKEY' --conn-password 'AWS_PASS'
          airflow connections add 'lpgs_gadi' --conn-type 'ssh' --conn-login 'USERNAME' --conn-password 'PASS'
          airflow connections add 'db_rep_writer_dev' --conn-type 'postgres' --conn-login 'USERNAME' --conn-password 'PASS'
          airflow connections add 'db_rep_writer_prod' --conn-type 'postgres' --conn-login 'USERNAME' --conn-password 'PASS'
          airflow connections add 'db_odc_reader' --conn-type 'postgres' --conn-login 'USERNAME' --conn-password 'PASS'
          airflow variables set 'm2m_api_password' '{"pass":"PASS"}'
          airflow variables set 'copernicus_api_password' '{"pass":"PASS"}'
          airflow variables set 'google_analytics' '{"pass":"PASS"}'
          airflow variables set 'rep_currency_product_list_nci_odc' '[{"product_id":"product_id","rate":"daily"}]'
          airflow info
          airflow dags list -v

          # check if list_dags has error if yes exit CI
          airflow dags list -v > dag_list.txt
          if grep -rniw './dag_list.txt' -e 'Error'; then
            echo "error found"
            exit 1
          fi

      - name: run tests on dags
        run: |
          # export AIRFLOW__CORE__UNIT_TEST_MODE="True"
          export PYTHONPATH="${PYTHONPATH:+${PYTHONPATH}:}${PWD}/dags"
          pip install pylint==2.7.2 pytest==6.2.2 pylint-airflow
          pytest tests/dag_structure
